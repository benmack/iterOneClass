---
title: "IterativeOcc"
author: "knecmab"
date: "Sunday, February 22, 2015"
output:
html_document:
dev: pdf
number_sections: yes
toc: yes
pdf_document:
toc: yes
---

```{r setup, echo=FALSE, message=FALSE}
require(knitr)
require(kfigr)
opts_chunk$set(message=FALSE, warning=FALSE)
```

# Introduction

# Required input

The following packages are required:

```{r, loadPackages}
require(doParallel)
require(kernlab)
require(pROC)
require(gdalUtils)

require(iterOneClass)
require(rasterTiled)

register_backend <- 
  function(cl=NULL, nCores=NULL) {
    require(foreach)
    require(doParallel)
    
    if (is.null(nCores))
      nCores <- detectCores()
      try(stopCluster(cl))
      cl <- makeCluster(nCores)
      registerDoParallel(cl)
      return(cl)
  }

```

And a list of settings:

```{r, settings}
sttngs <- list(nTrainUn = 100,
               iterMax = "noChange+3",
               indepUn = 0.5,
               kFolds = 10,
               seed = 222,
               baseDir = paste0("D:/github/iterOneClass/demo/results_s", 222),
               nPixelsPerTile = 10000,
               expand = 3, 
               minPppAtLw=TRUE, 
               sieve=25)
nCores <- 3
```

Finally, some data:

```{r, loadData}
dat <- list(
  U=rasterTiled(get_banana()$x, 
                 nPixelsPerTile=sttngs$nPixelsPerTile),
  P=banana_trn_pos(n_trn_pos=20, sttngs$seed),
  PN=banana_tst_set(n=5000, seed=sttngs$seed))
```

```{r}
sigest(dat$P, frac=1)
sigest(as.matrix(dat$PN[, -1]), frac=1)
```


```{r}
dat$Utr <- sample_rasterTiled(dat$U, size=sttngs$nTrainUn, 
                          seed=sttngs$seed)
plot(rbind(dat$P, dat$Utr), pch=c(1, 4)[rep(c(1, 2), c(nrow(dat$P), nrow(dat$Utr)))])
```

# Processing and analysis

## Run `iterativeOcc()`

```{r, runIterativeOcc, warning=FALSE}
cl <- makeCluster(detectCores())
registerDoParallel(cl)

iocc <- iterativeOcc(dat$P, dat$U, settings=sttngs, 
                     useSigest=T, smallIniGrid=T)
```


```{r}
finalIter <- which(diff(iocc$n_un_all_iters)==0)[1]

cl <- register_backend(cl, nCores)
modRows_unique <- get_unique_model(sttngs$baseDir, 1:finalIter)
sapply(modRows_unique, length)

subfolder_PN <- "PN"
cl <- register_backend(cl, 3)
ev_all <- evaluate_iocc(baseDir=sttngs$baseDir, iters=1:finalIter, 
                        refset=dat$PN, modRows=modRows_unique, 
                        subfolder=subfolder_PN)
sapply(ev_all, length)


```


```{r}
# cl <- makeCluster(detectCores())
# osocc <- one_step_occ(sttngs=sttngs, iter=4, test_set=PN,
#                       filename_U=U$raster@file@name,
#                       folder_out=paste0(sttngs$baseDir, "/iter-", 4, "/", "osocc/"))
```

### EXTEND GRID ?
```{r}
# i = 4
# subfolder_i4 <- "predU-5000_rand666-iter4"
# iocc.i <- get_ioccObj(sttngs$baseDir, iter=i)
# 
# max.puF <- get_max_puF(sttngs$baseDir, i, modRows_unique[[i]], 
#                        subfolder_i4)
# 
# iocc.i2$model$results[max.puF$modRow, "mxPuF"] <- max.puF$max.puF
# 
# iocc.i2$model$results
# iocc.i2$model$results[is.na(iocc.i2$model$results)] <- -1
# iocc.i2$model$metric = "mxPuF"
# plot_iocc(iocc.i2, what="grid")

```

## Benchmark


## Analyze the outcome


The number of unlabeled samples decreases and converges rapidly.
After the forth iteration no more samples can be classified as negatives
based on the conservative classification rule.

The processing time required per iteration also decreases, mainly due to the 
prediction time.

```{r out.width=c('400px', '400px'), fig_n_unlabeled-time, fig.show='hold'}
plot_iocc(iocc, what="n_unlabeled") 
plot_iocc(iocc, what="time")
```

The following plots show the model in the feature space,
the diagnostic histogram plot and the threshold dependent accuracy (from right to left column)
of the model selected per iteration. 
Note that the model shown is selected based on the `minPppAtLw` rule, **not** the `puF` or `puAuc`. 

```{r out.width=c('300px', '300px', '300px'), fig_fs-hist-eval_allIters, fig.show='hold'}
for (iter in 1:6) { 
  plot_iocc(sttngs$baseDir, iter, what="featurespace") 
  plot_iocc(sttngs$baseDir, iter, what="hist") 
  plot_iocc(sttngs$baseDir, iter, what="eval")
  }
```

## Selection of the final model

Selection of the final model should be reviewed by an analyst 
familiar with the nature and challenges of one-class classification.
Of course, the difficulty is to select a model without a exhaustive 
and representative reference data set.

In the follwing, methods are presented which facilitate the selection 
of the final model.

First, the diagnostic histogram plots can be analyzed and checked on 
their palausibility.
Let us investigate the 4th iteration, i.e. the first iteration 
no unlabeled samples have been converted to negatives.

```{r, get_ioccObj}
iocco <- get_ioccObj(sttngs$baseDir, iter=4)
```

Recall that the model selection so far is based on the 
probability of negative prediction at the lower-whisker threshold 
of the positive hold out predictions  (`pnpAtLw`). This is not necessarily 
an appropriate selection criteria for the final model.

```{r, modelPosition}
iocco$model$metric
modelPosition(iocco$model)
```

### Manual selection by investigating diagnostic histogram plots

The diagnostic histogram provide evidentce on the 
separability of the classes and the plausibility of the 
model. Let us first consider the models selected by the 
PU-performance metrics (`puF`and `puAuc`).

The model position of these models can be derived as follows:

```{r, puF-puAuc}
mp.puF <- modelPosition(iocco$model, modRank=1, by='puF')
mp.puF

mp.puAuc <- modelPosition(iocco$model, modRank=1, by='puAuc')
mp.puAuc
```

With the function `get_ioccObj` an iterativeOcc object can be 
created for the models.

```{r, get_ioccObj_puF-puAuc}
cl <- makeCluster(detectCores())
registerDoParallel(cl)

modRows <- c(mp.puF$row, mp.puAuc$row)
for (modRow in modRows)
  get_ioccObj(sttngs$baseDir, 4, modRow=modRow, test_set=PN)
```

The data is also saved in the working directory and it is now 
possible to plot the diagnostic histogram plot for the 
respective models:

```{r out.width=c('300px', '300px', '300px'), fig_fs-hist-eval_iter4-puF-puAuc, fig.show='hold'}
modRows
for (modRow in modRows) { 
  plot_iocc(sttngs$baseDir, 4, what="featurespace", modRow) 
  plot_iocc(sttngs$baseDir, 4, what="hist", modRow) 
  plot_iocc(sttngs$baseDir, 4, what="eval", modRow)
  }
```

The model selected by the metric `puF` (first row) seems more suitable 
than the model selected by the metric `puAuc` (second row).
However, the two box-plots of the positive data 
(the lightblue beeing the predictions on the positive training data 
(NOT hold-out!) 
and the dark blue the hold-out predictions of the same data).

The fact that the hold-out predictons (dark-blue boxplot) have 
significant lower predictive values implies a relatively complex 
model.
The following model illustrates a very complex model
(see the feature space plot on the right) and the resulting 
difference between the boxplots:

```{r out.width=c('400px', '400px'), overfitting, fig.show='hold'}
plot_iocc(sttngs$baseDir, 4, what="hist", modRow=160)
plot_iocc(sttngs$baseDir, 4, what="featurespace", modRow=160)
```

The `puF` model is not necessarily overfitting, nevertheless, if a model 
can be found which seems similarly separable (i.e. well separated 
histogram modes) but less complex (i.e. less difference between the
blue and light-blue boxplots) it should be prefered.

In order to get a fast overview over the models we can build 
the histogram plots for all the models. 
To be faster it males sense to only consider non-identical models.
Here, a identical model is identified by comparing the hold-out 
predictions. If they are identical the model is assumed to be identical.

```{r}
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

modRows_unique <- get_unique_model(sttngs$baseDir, 1:4)
```

For anoverview it is also sufficient to  build the histogram 
from a subset of unlabeled samples. As a major part of the 
unlabeled samples have already been 
rejected and classified as negatives a relatively small sample should 
be sufficient to represent well the distribution of the remaining 
unlabeled data.

The following function builds the diagnostic histogram plots with
a subset of 5000 unlabeled samples for all models and writes the PDFs 
to the folder 
`<baseDir>/iter-4/predU-5000`:

```{r}
# get_all_diagnostics(sttngs$baseDir, iter, unSub=5000,
#                     seed=sttngs$seed*2)
```

Screening these plots looking for the aforementioned simple patterns 
is a relatively fast task and could lead to the following alternative:

```{r}
# plot_iocc(sttngs$baseDir, iter, what="hist", modRow=86)
# plot_iocc(sttngs$baseDir, iter, what="featurespace", modRow=86)
# plot_iocc(sttngs$baseDir, iter, "eval")
```

### Automatic selection based on threshold-dependent PU-performance metrics

Get a sample of unlabeled pixels from the remaining unlabeled 
pixels.

```{r}
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

subfolder_i4 <- "predU-5000_rand666-iter4"
unSub_i4 <- get_Usub(sttngs$baseDir, 4, 5000, sttngs$seed*3,
                     subfolder_i4)
```

Get the predictions for these samples of all models from iterations 1 to 4. 

```{r}
for (i in 1:4) {
  predict_Usub(iocc, iter=i, unSub=unSub_i4, 
               modRows=modRows_unique[[i]], 
               folder_suffix=subfolder_i4)
  }
```

Use this set to compute threshold dependent accuracies for 
all models of all iterations.

```{r}
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

tdp <- list()
for (i in 1:4) {
  tdp[[i]] <- th_dep_performance(sttngs$baseDir, i,
                                 modRows=modRows_unique[[i]],
                                 folder_U=subfolder_i4, 
                                 return_val=TRUE)
  }
str(tdp)
```

**Problem**: 

Sometimes `puF` values get very high at high threshold values.
Outliers bias the results.

**Solutions**:

* Constrain threshold range at median of positive hold-out predictions   

* Remove outliers

```{r}
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

hops <- get_puSet(sttngs$baseDir, 4, modRow=modRows_unique[[4]])
pos_out <- lapply(hops, function(x) {bp <- boxplot(x$pos, plot=FALSE)
                                     as.numeric(names(bp$out[bp$out<bp$stats[3]]))})
table(unlist(pos_out))
```


```{r}
# rm_pos <- c(19, 16)
# th_dep_performance(sttngs$baseDir, iter=4, modRow=85,
#                    folder_U=subfolder_i4, rm_pos=rm_pos,
#                    return_val=TRUE, overwrite=TRUE, plot=TRUE)
```

List the models according to the maximum `puF`. 

```{r}
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
max.puF <- get_max_puF(sttngs$baseDir, 1:4, modRows_unique, 
                       subfolder_i4)
head(max.puF)
```

### Test set

```{r}
subfolder_pn <- "predPN-5000"

stopCluster(cl)
cl <- makeCluster(detectCores())

for (i in 1:4)
  predict_Usub_withRej(sttngs$baseDir, iter=i, unSub=PN[, -1], 
               modRows=modRows_unique[[i]], 
               folder_suffix=subfolder_pn)

registerDoParallel(cl)
ev_all <- evaluate_iocc(baseDir=sttngs$baseDir, iters=1:4, 
                        refset=PN, modRows=modRows_unique, 
                        subfolder=subfolder_pn)

ev_all
```


```{r}

```

Combine ``max.puF`` and ``max.kappa`` from ``ev.all``
```{r}
for (i in 1:nrow(max.puF)) {
  iter <- max.puF$iter[i]
  el <- which(modRows_unique[[iter]]==max.puF$modRow[i])
  cat("iter/el: ", iter, "/", el, "\n")
  ev <- ev_all[[iter]][[el]]
  max.puF[i, "max.k"] <- max(ev@kappa)
}
head(max.puF)
tail(max.puF)
plot(max.puF$max.k, max.puF$max.puF, 
     pch=21, bg=c("#d7191c", "#fdae61", "#abd9e9", "#2c7bb6")[max.puF$iter])

```

